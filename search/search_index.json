{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the SCoRe user documentation","text":"<p>This site intends to help a user find the resources for his/her needs.</p> <p>Find the resources for you needs.</p> <p> </p>"},{"location":"CODE_OF_CONDUCT/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the   overall community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or   advances of any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email   address, without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"CODE_OF_CONDUCT/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at marcus.lundberg@uppmax.uu.se. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"CODE_OF_CONDUCT/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"CODE_OF_CONDUCT/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"CODE_OF_CONDUCT/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"CODE_OF_CONDUCT/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior,  harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"CONTRIBUTING/","title":"Contributing","text":"<p>Thanks for considering to contribute and reading this!</p> <p>Here we discuss how to contribute:</p> <ul> <li>Spoken text, e.g. ideas, feedback, messages, etc.   and are written in English.</li> <li>Code, e.g. textual changes where the text is formatted in Markdown</li> </ul>"},{"location":"CONTRIBUTING/#improvements-to-the-courses-page","title":"Improvements to the Courses page","text":"<p>The Courses page scrapes the websites of course providers, using the <code>scoreto</code> R package.</p> <p>Not all course providers give information about their courses in a machine-friendly way. This can cause courses to be (1) not being scraped, or (2) being displayed incorrectly.</p> <p>Things that will be fixed are:</p> <ul> <li>A course is not scraped from a provider's site</li> <li>A course is displayed unexpectedly weird</li> </ul> <p>Things that will not be fixed are:</p> <ul> <li>A course URL, as copied from a provider's site, is broken.   These URLs are indicated in the table with a  emoji</li> </ul>"},{"location":"CONTRIBUTING/#spoken-text","title":"Spoken text","text":"<p>Spoken text are ideas, feedback, messages, etc. and are written in English.</p> <p>For ideas or feedback, create an issue. These issues will be discussed in a meeting and/or below that issue.</p> <p>Ideas that our site's visitors find the computational resources they need is likely to be accepted.</p>"},{"location":"CONTRIBUTING/#code","title":"Code","text":"<p>We welcome any contribution that helps our site's visitors find the computational resources they need.</p>"},{"location":"compute/","title":"Compute","text":"<p>There are multiple types of resources you may need. This page is about finding a place to run heavy calculations on: it shows a flowchart how to determine the computational resource you can use, followed by an overview of all resources.</p> <pre><code>flowchart TD\n\n  subgraph researcher_on_sensitive_data[Researcher, sensitive data]\n    arrhenius_rs[Arrhenius]\n    bianca[Bianca]\n    cosmos_sens[COSMOS SENS or LUSEC]    \n    data_science_platform_rs[Data Science Platform]\n    lumi_rs[LUMI]\n    maja[Maja]\n    trusted_research_environment_rs[Trusted Research Environment]\n\n    question_custom_rs[Need a custom setup or a GPU-focused system?]\n    question_custom_rs --&gt; |Yes| data_science_platform_rs\n    question_custom_rs --&gt; |No| question_where_rs\n\n    question_where_rs[Where are you located?]\n    question_where_rs --&gt; |Sweden| bianca\n    question_where_rs --&gt; |Lund| cosmos_sens\n    question_where_rs --&gt; |Gothenburg| trusted_research_environment_rs\n\n    bianca -.-&gt; |Future| maja\n    bianca -.-&gt; |Future| arrhenius_rs\n\n  end\n\n  subgraph researcher_on_regular_data[Researcher, regular data]\n    alvis[Alvis]\n    arrhenius_rr[Arrhenius]\n    berzelius[Berzelius]\n    cosmos[COSMOS]\n    dardel[Dardel]\n    data_science_platform_rr[Data Science Platform]\n    kebnekaise[Kebnekaise]\n    lumi[LUMI]\n    pelle[Pelle]\n    rackham[Rackham]\n    sigma[Sigma]\n    tetralith[Tetralith]\n    vera[Vera]\n\n    question_custom_rr[Need a custom setup?]\n    question_custom_rr --&gt; |Yes| data_science_platform_rr\n    question_custom_rr --&gt; |No| question_very_heavy_compute_rr\n\n    question_very_heavy_compute_rr[Need very heavy compute?]\n    question_very_heavy_compute_rr --&gt; |Yes| lumi\n    question_very_heavy_compute_rr --&gt; |No| question_ai\n\n    question_ai[Are you working with AI?]\n    question_ai --&gt; |Yes| question_kaw\n    question_ai --&gt; |No| question_where_rr\n\n    question_kaw[Are you affiliated with KAW, DDLS, WASP, or similar?]\n    question_kaw --&gt; |Yes| berzelius\n    question_kaw --&gt; |No| alvis\n\n    question_where_rr[Where are you located?]\n    question_where_rr --&gt; |Lund| cosmos\n    question_where_rr --&gt; |Sweden| dardel\n    question_where_rr --&gt; |Lund| kebnekaise\n    question_where_rr --&gt; |Sweden| tetralith\n    question_where_rr --&gt; |Link\u00f6ping| sigma\n    question_where_rr --&gt; |Gothenburg| vera\n    question_where_rr --&gt; |Uppsala| rackham\n\n    rackham -.-&gt; |Future| pelle\n    rackham -.-&gt; |Future| arrhenius_rr\n\n  end\n\n  subgraph anyone_on_any_data[Anyone, any data]\n    aa_data_science_platform[Data Science Platform]\n  end\n\n  job_question[Are you a researcher?]\n\n  job_question --&gt; |No| anyone_on_any_data\n  job_question --&gt; |Yes| sensitivity_question\n\n  sensitivity_question[Do you work with sensitive data?]\n  sensitivity_question --&gt; |Yes| researcher_on_sensitive_data\n  sensitivity_question --&gt; |No| researcher_on_regular_data\n\n  researcher_on_sensitive_data ~~~ researcher_on_regular_data </code></pre> Why is this a useful resource? <p>This page is the only page that combines all the computational resources of all the different providers.</p> How is this list generated and updated? <p>On a daily basis, the <code>update_content.yaml</code> continuous integration script checks the websites of the course providers and updates the list, using the <code>scoreto</code> R package.</p> A compute provider is missing! <p>If a compute provider is missing, please contribute or contact us.</p> My compute resource is absent! <p>If your compute resource is absent, please contribute or contact us.</p> My compute resource can be displayed better! <p>If your compute resource can be displayed better, please contribute or contact us.</p> How can I read this data is a machine-friendly format? <p>This information can be downloaded as a <code>.csv</code> from the <code>scoreto</code> R package.</p> HPC cluster name Type of computation Type of data User fee Accessible for Center(s) Alvis AI Regular Free Swedish researchers Berzelius General purpose Regular Free Link\u00f6ping researchers Bianca General purpose Sensitive Free Swedish researchers COSMOS General purpose Regular Free Lund researchers COSMOS SENS General purpose Sensitive Free Lund researchers Dardel General purpose Regular Free Swedish researchers Data Science Platform Any Any User fee Anyone Kebnekaise General purpose Regular Free Ume\u00e5 researchers LUMI General purpose Regular Free Swedish researchers Rackham General purpose Regular Free Uppsala researchers Sigma General purpose Regular Free Link\u00f6ping researchers Tetralith General purpose Regular Free Swedish researchers Trusted research environment General purpose Any Free Gothenburg researchers Vera General purpose Regular Free Gothenburg researchers"},{"location":"compute_1/","title":"Compute","text":"<p>There are multiple types of resources you may need. This page is about finding a place to run heavy calculations on: it shows a flowchart how to determine the computational resource you can use, followed by an overview of all resources.</p> <pre><code>flowchart TD\n\n  subgraph researcher_on_sensitive_data[Researcher, sensitive data]\n    arrhenius_rs[Arrhenius]\n    bianca[Bianca]\n    cosmos_sens[COSMOS SENS or LUSEC]    \n    data_science_platform_rs[Data Science Platform]\n    lumi_rs[LUMI]\n    maja[Maja]\n    trusted_research_environment_rs[Trusted Research Environment]\n\n    question_custom_rs[Need a custom setup or a GPU-focused system?]\n    question_custom_rs --&gt; |Yes| data_science_platform_rs\n    question_custom_rs --&gt; |No| question_where_rs\n\n    question_where_rs[Where are you located?]\n    question_where_rs --&gt; |Sweden| bianca\n    question_where_rs --&gt; |Lund| cosmos_sens\n    question_where_rs --&gt; |Gothenburg| trusted_research_environment_rs\n\n    bianca -.-&gt; |Future| maja\n    bianca -.-&gt; |Future| arrhenius_rs\n\n  end\n\n  subgraph researcher_on_regular_data[Researcher, regular data]\n    alvis[Alvis]\n    arrhenius_rr[Arrhenius]\n    berzelius[Berzelius]\n    cosmos[COSMOS]\n    dardel[Dardel]\n    data_science_platform_rr[Data Science Platform]\n    kebnekaise[Kebnekaise]\n    lumi[LUMI]\n    pelle[Pelle]\n    rackham[Rackham]\n    sigma[Sigma]\n    tetralith[Tetralith]\n    vera[Vera]\n\n    question_custom_rr[Need a custom setup?]\n    question_custom_rr --&gt; |Yes| data_science_platform_rr\n    question_custom_rr --&gt; |No| question_very_heavy_compute_rr\n\n    question_very_heavy_compute_rr[Need very heavy compute?]\n    question_very_heavy_compute_rr --&gt; |Yes| lumi\n    question_very_heavy_compute_rr --&gt; |No| question_ai\n\n    question_ai[Are you working with AI?]\n    question_ai --&gt; |Yes| question_kaw\n    question_ai --&gt; |No| question_where_rr\n\n    question_kaw[Are you affiliated with KAW, DDLS, WASP, or similar?]\n    question_kaw --&gt; |Yes| berzelius\n    question_kaw --&gt; |No| alvis\n\n    question_where_rr[Where are you located?]\n    question_where_rr --&gt; |Lund| cosmos\n    question_where_rr --&gt; |Sweden| dardel\n    question_where_rr --&gt; |Lund| kebnekaise\n    question_where_rr --&gt; |Sweden| tetralith\n    question_where_rr --&gt; |Link\u00f6ping| sigma\n    question_where_rr --&gt; |Gothenburg| vera\n    question_where_rr --&gt; |Uppsala| rackham\n\n    rackham -.-&gt; |Future| pelle\n    rackham -.-&gt; |Future| arrhenius_rr\n\n  end\n\n  subgraph anyone_on_any_data[Anyone, any data]\n    aa_data_science_platform[Data Science Platform]\n  end\n\n  job_question[Are you a researcher?]\n\n  job_question --&gt; |No| anyone_on_any_data\n  job_question --&gt; |Yes| sensitivity_question\n\n  sensitivity_question[Do you work with sensitive data?]\n  sensitivity_question --&gt; |Yes| researcher_on_sensitive_data\n  sensitivity_question --&gt; |No| researcher_on_regular_data\n\n  researcher_on_sensitive_data ~~~ researcher_on_regular_data </code></pre> Why is this a useful resource? <p>This page is the only page that combines all the computational resources of all the different providers.</p> How is this list generated and updated? <p>On a daily basis, the <code>update_content.yaml</code> continuous integration script checks the websites of the course providers and updates the list, using the <code>scoreto</code> R package.</p> A compute provider is missing! <p>If a compute provider is missing, please contribute or contact us.</p> My compute resource is absent! <p>If your compute resource is absent, please contribute or contact us.</p> My compute resource can be displayed better! <p>If your compute resource can be displayed better, please contribute or contact us.</p> How can I read this data is a machine-friendly format? <p>This information can be downloaded as a <code>.csv</code> from the <code>scoreto</code> R package.</p>"},{"location":"compute_2/","title":"Compute 2","text":"HPC cluster name Type of computation Type of data User fee Accessible for Center(s) Alvis AI Regular Free Swedish researchers Berzelius General purpose Regular Free Link\u00f6ping researchers Bianca General purpose Sensitive Free Swedish researchers COSMOS General purpose Regular Free Lund researchers COSMOS SENS General purpose Sensitive Free Lund researchers Dardel General purpose Regular Free Swedish researchers Data Science Platform Any Any User fee Anyone Kebnekaise General purpose Regular Free Ume\u00e5 researchers LUMI General purpose Regular Free Swedish researchers Rackham General purpose Regular Free Uppsala researchers Sigma General purpose Regular Free Link\u00f6ping researchers Tetralith General purpose Regular Free Swedish researchers Trusted research environment General purpose Any Free Gothenburg researchers Vera General purpose Regular Free Gothenburg researchers"},{"location":"contact_us/","title":"Contact us","text":"<p>Sometimes there is something incorrect us or missing. Please let us know!</p> <p>You can contact us:</p> Contact method Features Create an issue Best for asking a question publicly, requires a GitHub account Create a pull request Best for improving the course content, requires a GitHub account Send an email Best for asking a question privately, invisible to others <p>Sending an email:</p> <ul> <li>Jonas S\u00f6derberg (head of SCoRe)</li> <li>Rich\u00e8l Bilderbeek</li> </ul>","tags":["contact","email","talk","find","speak","meet"]},{"location":"course_materials/","title":"Course materials","text":"<p>There are multiple types of resources you may need. Knowledge is a resource too. This page shows the places to increase your knowledge by reading self-study material.</p>","tags":["courses","training","workshops","teach","teaching","learning","knowledge"]},{"location":"course_materials/#list-of-course-materials","title":"List of course materials","text":"Why is this a useful resource? <p>This page is the only page that combines all the course materials of all the different providers.</p> How was this overview created? <p>This overview was created by going through providers' sites and copying the resources they linked to.</p> Resource Topic Codecademy Computer science and programming languages ENCCS HPC computing Learn X in Y minutes Programming languages LUMI LUMI and HPC computing SCC Swedish Science Cloud, e.g. deploying an interactive website","tags":["courses","training","workshops","teach","teaching","learning","knowledge"]},{"location":"courses/","title":"Courses","text":"<p>There are multiple types of resources you may need. Knowledge is a resource too. This page shows the places to increase your knowledge in a learning environment, as provided by courses and training.</p> Why is this a useful resource? <p>This page is the only page that combines all the courses of all the different providers.</p> <p>Additionally, it can be used to</p> <ul> <li>Detect broken links to courses, as indicated by the    emoji</li> <li>Detect inconsistencies when the same course is advertised by multiple   providers, as the same course will be displayed multiple times</li> </ul> How is this list generated and updated? <p>On a daily basis, the <code>update_content.yaml</code> continuous integration script checks the websites of the course providers and updates the list, using the <code>scoreto</code> R package.</p> What does the  mean? <p>It means that the course provider has supplied a broken course URL.</p> A course provider is missing! <p>If a course provider is missing, please contribute or contact us.</p> <p>To be a course provider, one needs to provide for a learning environment with a live and human teacher. If a course providers provides an online self-study learning environment, it is listed at the course materials page.</p> My course is missing! <p>If your course is missing, please contribute or contact us.</p> My course can be displayed better! <p>If your course can be displayed better, please contribute or contact us.</p> My course is displayed twice! <p>The same course can be listed by multiple providers. If so, this page will show the same course muliple times. This is intentional: it allows a course provider to check if all course information is shared identical across the providers.</p> How can I read this data is a machine-friendly format? <p>Most of this information can be downloaded as a <code>.csv</code>. The only difference is there is no indicator that a URL is broken.</p> <p>If you could use this information in the <code>.csv</code> file, please contribute or contact us.</p> From To Course name Course site Provider site Provider name 2025-11-03 2025-11-07 Quantum Autumn School 2025 Course site Provider site 2025-11-03 2025-11-07 Quantum Autumn School 2025 Course site Provider site 2025-11-03 2025-11-07 Introduction to Git Course site Provider site 2025-11-03 2025-11-14 Proteomics by Mass Spectrometry: When and How Course site Provider site 2025-11-03 2026-01-18 Introduction to Bioinformatics Course site Provider site 2025-11-03 2026-01-18 Advanced Plant Breeding and Genetic Resources Course site Provider site 2025-11-10 2025-11-10 Selecting Software Modules Course site Provider site 2025-11-10 2025-11-14 Physiology of ion channels and their role in disease Course site Provider site 2025-11-11 2025-11-14 Hands-on Course in Epigenomics Course site Provider site 2025-11-14 2025-11-14 Log in and transfer files to/from HPC Clusters Course site Provider site 2025-11-17 2025-11-21 Introduction to bioinformatics using NGS data Course site Provider site 2025-11-18 2025-11-20 Introduction to GPU programming Course site Provider site 2025-11-18 2025-11-20 [Webinars] Introduction to GPU programming Course site Provider site 2025-11-18 2025-11-18 Bianca In-Depth Workshop/Hackathon: Improve Your Handling of Sensitive Research Data Course site Provider site 2025-11-20 2026-02-17 Statistical methods in animal breeding Course site Provider site 2025-11-24 2025-11-28 Introduction to Python - with application to bioinformatics Course site Provider site 2025-11-25 2025-11-25 AITO webinar Course site Provider site 2025-11-25 2025-11-27 GPU programming: when, why and how? Course site Provider site 2025-11-25 2025-11-27 GPU programming: when, why and how? Course site Provider site 2025-11-25 2025-11-25 Running jobs on clusters Course site Provider site 2025-11-25 2025-11-25 Running jobs on clusters Course site Provider site 2025-11-27 2025-11-28 Using Python in an HPC environment part 2 Course site Provider site 2025-11-28 2025-11-28 Introduction to running Python in HPC + Using Python in an HPC environment Course site Provider site 2025-12-01 2025-12-05 R Programming Foundation For Data Analysis Course site Provider site 2025-12-01 2025-12-02 Using Python in an HPC environment part 1 Course site Provider site 2025-12-03 2025-12-03 Introduction seminar for Alvis users Course site Provider site 2025-12-04 2025-12-05 Intermediate Bash, aka Command Line 201 Course site Provider site 2025-12-08 2025-12-12 Practical Course at UCEM - TEM sample preparation for life science Course site Provider site 2025-12-31 2025-12-31 Biodiversity Genomics Europe + SciLifeLab Digital Hi-C Course - Self Paced Course site Provider site 2025-12-31 2025-12-31 Open Science in the Swedish Context - Materials Course site Provider site 2026-01-19 2026-01-30 Data handling and high-quality illustrations for publications Course site Provider site 2026-03-11 2026-03-16 Biologically Informed Neural Networks (BINNs): Principles and Practice Course site Provider site"},{"location":"efficiency/","title":"Efficiency","text":"<p>This page is a stub</p> <p>As of now, this page is incomplete, possibly incorrect and open for contributions.</p> <p>There are multiple types of resources you may need. This page is about using HPC resources efficiently, i.e. how to schedule your HPC jobs optimally.</p> <p>What is not included is how to profile computer code, as this can be done on any local computer, instead of the heavier compute resources.</p> How was this overview created? <p>This overview was created by going through all HPC cluster centers and merging the material they provided regarding this topic.</p> <p>This is the material found:</p> <p></p> HPC cluster name Guide on how to improve efficiency Center(s) Alvis None found Bianca UPPMAX <code>jobstats</code> page COSMOS None found COSMOS SENS None found Dardel None found Data Science Platform None found Kebnekaise <code>job-usage</code> LUMI No guide Rackham UPPMAX <code>jobstats</code> page Sigma None found Tetralith None found Trusted research environment None found Vera None found My center's guide is not linked to! <p>If your center's guide is not linked to, please contribute or contact us.</p> <p>Additionally, searching for this topic, these sources were found too:</p> <ul> <li>Southern Methodist University best practices guide</li> <li>Stack Overflow post on how to get the CPU and memory usage</li> <li>Blog post on using <code>seff</code> and <code>reportseff</code></li> </ul> <p>From that, all material was merged into one.</p> <p>Below is a general strategy to effectively use your HPC resources. How this looks like in practice, depends on the tool available on your HPC cluster.</p> <pre><code>flowchart TD\n  obtain_data[Obtain CPU and memory usage of a job]\n  lower_limit_based_on_memory(Book enough memory)\n  limited_by_cpu(For that amount of cores, would runtime by limited by CPU?)\n  lower_limit_based_on_cpu(Increase the number of cores, so that on average, the right amount of CPUs is booked)\n\n  done(Use that amount of cores)\n\n  add_one(Increase the number of cores by one for safety)\n\n  obtain_data --&gt; lower_limit_based_on_memory\n  lower_limit_based_on_memory --&gt; limited_by_cpu\n  limited_by_cpu --&gt; |no| add_one\n  limited_by_cpu --&gt; |yes| lower_limit_based_on_cpu\n  lower_limit_based_on_cpu --&gt; done\n  add_one --&gt; done</code></pre> Why not look at CPU usage? <p>Because CPU is more flexible.</p> <p>For example, imagine a job with a short CPU spike, that can be processed by 16 CPUs. If 1 core has enough memory, use 1 core of memory: the CPU spike will be turned into a 100% CPU use (of that one core) for a longer duration.</p> <p>The first step, 'Obtain CPU and memory usage of a job' depends on your HPC cluster:</p> HPC cluster name Tool and guide Center(s) Alvis Using a graphical representation Bianca Using <code>jobstats</code> COSMOS Using <code>sacct</code> COSMOS SENS Using <code>sacct</code> Dardel Using <code>seff</code> Data Science Platform ? Kebnekaise Using <code>seff</code> LUMI Using <code>seff</code> Pelle Using <code>sacct</code> Rackham Using <code>jobstats</code> Sigma Using <code>seff</code> Tetralith Using <code>seff</code> Trusted research environment ? Vera Using a graphical representation Need a worked-out example? <p>Worked-out examples can be found on each page specific to the tool used.</p>"},{"location":"efficiency_using_graphical_representation/","title":"Efficiency using a graphical representation","text":"<p>There are multiple tools for using your HPC resources efficiently you may need.</p> <p>This page is about using your HPC resources efficiently using graphical representation of the resources used for each job run via Slurm, such as done by C3SE.</p> <p>Here is the general strategy to effectively use your HPC resources:</p> Want to see a video? <p>Watch the YouTube video obtain the CPU and memory usage of a job using <code>Grafana</code> to see how to do so.</p> <p>Watch the YouTube video Efficient HPC resource use, using Slurm and Grafana to see how the reasoning of this strategy works out.</p> <pre><code>flowchart TD\n  obtain_data[Obtain CPU and memory usage of a job]\n  lower_limit_based_on_memory(Book enough memory)\n  limited_by_cpu(For that amount of cores, would runtime by limited by CPU?)\n  lower_limit_based_on_cpu(Increase the number of cores, so that on average, the right amount of CPUs is booked)\n\n  done(Use that amount of cores)\n\n  add_one(Increase the number of cores by one for safety)\n\n  obtain_data --&gt; lower_limit_based_on_memory\n  lower_limit_based_on_memory --&gt; limited_by_cpu\n  limited_by_cpu --&gt; |no| add_one\n  limited_by_cpu --&gt; |yes| lower_limit_based_on_cpu\n  lower_limit_based_on_cpu --&gt; done\n  add_one --&gt; done</code></pre> Why not look at CPU usage? <p>Because CPU is more flexible.</p> <p>For example, imagine a job with a short CPU spike, that can be processed by 16 CPUs. If 1 core has enough memory, use 1 core of memory: the CPU spike will be turned into a 100% CPU use (of that one core) for a longer duration.</p> <p>To obtain the CPU and memory usage of a job using <code>Grafana</code>:</p> <pre><code>job_stats.py [job_number]\n</code></pre> <p>for example:</p> <pre><code>job_stats.py 12696175\n</code></pre> How does that look like? <p>This will produce output such as this:</p> <p></p> Need a worked-out example? <p></p> <p>Book enough memory</p> <p>We see that 568 MiB is used of 62.5 GiB with 16 cores. We need <code>((568 / 62500) * 16) =</code> 0.15 cores to have enough memory. In practice, this means 1 core.</p> <p>For that amount of cores, would runtime by limited by CPU?</p> <p>We see that on average 1.35 cores are needed. So yes, this means runtime is limited by CPU, so we book 2 CPU cores.</p> <p>Increase the number of cores by one for safety</p> <p>This results in 3 CPU cores.</p> Need another worked-out example? <p>Book enough memory</p> <p>.</p> <p>For that amount of cores, would runtime by limited by CPU?</p> <p>.</p> <p>Increase the number of cores, so that on average the right amount of CPUs are booked</p> <p>.</p> <p>Sometimes, however, it is inevitable to use resources inefficiently.</p>"},{"location":"efficiency_using_graphical_representation/#examples","title":"Examples","text":"<p>Here are some examples of how inefficient jobs can look and what you can do to make them more efficient.</p>"},{"location":"efficiency_using_graphical_representation/#inefficient-job-example-1-booking-too-much-cores","title":"Inefficient job example 1: booking too much cores","text":"<pre><code>Job ID: 12696175\nCluster: dardel\nUser/Group: aletyner/aletyner\nState: COMPLETED (exit code 0)\nNodes: 1\nCores per node: 160\nCPU Utilized: 00:00:03\nCPU Efficiency: 0.00% of 1-23:22:40 core-walltime\nJob Wall-clock time: 00:17:46\nMemory Utilized: 4.35 GB\nMemory Efficiency: 3.17% of 137.19 GB (878.00 MB/core)\nThe task which had the largest memory consumption differs by 102.24% from the average task max memory consumption\n</code></pre> <p>Here booking 7 cores is considered okay.</p> <p>Book enough memory</p> <p>In this job, only 3.17% of the memory of was used. 3.17% of 160 scheduled cores is 5.072 core. In practice, this will be 6 cores.</p> <p>For that amount of cores, would runtime by limited by CPU?</p> <p>The answer is 'no': we see a CPU efficiency of 0.00% (i.e. 0.0049% or lower). Hence, using reducing the number of cores to 3.17% will still be enough for the CPU.</p> <p>Increase the number of cores by one for safety</p> <p>This means booking 7 cores is recommended.</p>"},{"location":"efficiency_using_jobstats/","title":"Efficiency using <code>jobstats</code>","text":"<p>There are multiple tools for using your HPC resources efficiently you may need. This page is about using your HPC resources efficiently using the <code>jobstats</code> tool.</p> <p>Here is the general strategy to effectively use your HPC resources:</p> Want to see a video? <p>Watch the YouTube video obtain the CPU and memory usage of a job using <code>jobstats</code> to see how to do so.</p> <p>Watch the YouTube video Efficient HPC resource use, using Slurm and jobstats to see how the reasoning of this strategy works out.</p> <pre><code>flowchart TD\n  obtain_data[Obtain CPU and memory usage of a job]\n  lower_limit_based_on_memory(Pick the number of cores to have enough memory)\n  limited_by_cpu(For that amount of cores, would runtime by limited by CPU?)\n  lower_limit_based_on_cpu(Increase the number of cores, so that on average, the right amount of CPUs is booked)\n\n  done(Use that amount of cores)\n\n  add_one(Increase the number of cores by one for safety)\n\n  obtain_data --&gt; lower_limit_based_on_memory\n  lower_limit_based_on_memory --&gt; limited_by_cpu\n  limited_by_cpu --&gt; |no| add_one\n  limited_by_cpu --&gt; |yes| lower_limit_based_on_cpu\n  lower_limit_based_on_cpu --&gt; done\n  add_one --&gt; done</code></pre> Why not look at CPU usage? <p>Because CPU is more flexible.</p> <p>For example, imagine a job with a short CPU spike, that can be processed by 16 CPUs. If 1 core has enough memory, use 1 core of memory: the CPU spike will be turned into a 100% CPU use (of that one core) for a longer duration.</p> <p>To obtain the CPU and memory usage of a job using <code>jobstats</code>:</p> <pre><code>jobstats --plot [job_id]\n</code></pre> <p>for example:</p> <pre><code>jobstats --plot 12345678\n</code></pre> <p>A plot is produced showing the resource use per node for a job that completed successfully and took longer than 5 minutes.</p> <p>The produced plot will be produced in the local folder with name <code>[cluster_name]-[project_name]-[user_name]-[jobid].png</code>, for example <code>rackham-uppmax1234-sven-876543.png</code>. Use any image viewer, e.g. <code>eog</code>.</p> <p>Each plot shows:</p> <ul> <li>detailed information in the title.</li> <li>CPU usage in blue</li> <li>current memory usage in solid black</li> <li>overall memory usage in dotted black (if available)</li> </ul> Need a worked-out example? <p></p> <p>Pick the number of cores to have enough memory</p> <p>The dotted black line hits the right-hand vertical axis at 1070%. This means that 11 cores (i.e. 1100%) would be enough for this job.</p> <p>For that amount of cores, would runtime by limited by CPU?</p> <p>The answer is 'no'. Having 11 cores would mean that most of the time only 10 are used. Only in the CPU spike at the end, the runtime is limited by CPU. This short time only has a minor impact on the runtime speed.</p> <p>Increase the number of cores by one for safety</p> <p>This means booking 12 cores is recommended.</p> Need another worked-out example? <p></p> <p>Pick the number of cores to have enough memory</p> <p>The dotted black line hits the right-hand vertical axis at 90%. This means that 1 core (i.e. 100%) would be enough for this job.</p> <p>For that amount of cores, would runtime by limited by CPU?</p> <p>The answer is 'yes'. Having 1 core would mean that around half the time there is too little CPU power. This has an effect.</p> <p>Increase the number of cores, so that on average the right amount of CPUs are booked</p> <p>This is around 8 cores (800%), as with that amount of cores:</p> <ul> <li>half of the time, there is 1 out of 7 cores booked,   that is 6 too much</li> <li>half of the time, there is 7 out of 13 cores booked,   that is 6 too little</li> </ul> <p>This is not an exact algorithm and all numbers from 2 to 9 cores can be considered okay.</p> <p>Sometimes, however, it is inevitable to use resources inefficiently, see the examples below</p> <p>No queue is possible</p> <p>If everyone followed these guidelines, there would probably not even be a queue most of the time!</p>"},{"location":"efficiency_using_jobstats/#examples","title":"Examples","text":"<p>Here are some examples of how inefficient jobs can look and what you can do to make them more efficient.</p>"},{"location":"efficiency_using_jobstats/#inefficient-job-example-1-booking-too-much-cores","title":"Inefficient job example 1: booking too much cores","text":"<p>Here booking 5 cores is considered okay.</p> <p>Pick the number of cores to have enough memory</p> <p>The dotted black line hits the right-hand vertical axis at 390%. This means that 4 cores (i.e. 400%) would be enough for this job.</p> <p>For that amount of cores, would runtime by limited by CPU?</p> <p>The answer is 'no'. Having 4 cores would mean that most of the time only 1 are used. Only for some CPU spikes, the runtime is limited by CPU. This short time only has a minor impact on the runtime speed.</p> <p>Increase the number of cores by one for safety</p> <p>This means booking 5 cores is recommended.</p>"},{"location":"efficiency_using_jobstats/#inefficient-job-example-2-booking-too-much-cores","title":"Inefficient job example 2: booking too much cores","text":"<p>This is one of the grayer areas: booking 2-9 cores is all considered reasonable.</p> <p>Pick the number of cores to have enough memory</p> <p>The dotted black line hits the right-hand vertical axis at 90%. This means that 1 core (i.e. 100%) would be enough for this job.</p> <p>For that amount of cores, would runtime by limited by CPU?</p> <p>The answer is 'yes'. Having 1 core would mean that around half the time there is too little CPU power. This has an effect.</p> <p>Increase the number of cores, so that on average the right amount of CPUs are booked</p> <p>This is around 8 cores (800%), as with that amount of cores:</p> <ul> <li>half of the time, there is 1 out of 7 cores booked,   that is 6 too much</li> <li>half of the time, there is 7 out of 13 cores booked,   that is 6 too little</li> </ul> <p>This is not an exact algorithm and all numbers from 2 to 9 cores can be considered okay.</p>"},{"location":"efficiency_using_jobstats/#inefficient-job-example-3","title":"Inefficient job example 3","text":"<p>Here booking 6 cores is considered okay.</p> <p>Pick the number of cores to have enough memory</p> <p>The dotted black line hits the right-hand vertical axis at 40%. This means that 1 core (i.e. 100%) would be enough for this job.</p> <p>For that amount of cores, would runtime by limited by CPU?</p> <p>The answer is 'yes'. Having 1 core would mean that most of the time our run is limited by CPU power. This has an impact on the runtime speed.</p> <p>Increase the number of cores, so that on average the right amount of CPUs are booked</p> <p>This is around 6 cores (600%), as with that amount of cores:</p> <ul> <li>most of the time, there is 6 out of 6 cores booked,   that is 0 too much</li> <li>only rarely, there is a little spike up or a bigger spike down</li> </ul> <p>There are no signs of anything slowing them down, as the line is very even.</p> <p>This jobs should either have been booked with 6 cores, or the program running should be told to use all 8 cores.</p>"},{"location":"efficiency_using_jobstats/#inefficient-job-example-4-slowdown","title":"Inefficient job example 4: slowdown","text":"<p>This job is using almost all of the cores it has booked, but there seems to be something holding them back. The uneven blue curve tells us that something is slowing down the analysis, and it's not by a constant amount.</p> <p>Usually this is how it looks when the filesystem is the cause of a slowdown. Since the load of the filesystem is constantly changing, so will the speed by which a job can read data from it also change.</p> <p>This job should try to copy all the files it will be working with to the nodes local harddrive before running the analysis, and by doing so not be affected by the speed of the filesystem.</p> <p>Please see the guide How to use the nodes own hard drive for analysis for more information.</p> <p>You basically just add 2 more commands to your script file and the problem should be solved.</p>"},{"location":"efficiency_using_jobstats/#inefficient-job-example-5","title":"Inefficient job example 5","text":"<p>This job has the same problem as the example above, but in a more extreme way.</p> <p>It's not uncommon that people book whole nodes out of habit and only run single threaded programs that use almost no memory. This job is a bit special in the way that it's being run on a high memory node, as you can see on the left Y-axis, that it goes up to 256 GB RAM. A normal node on Milou only have 128GB. These high memory nodes are only bookable of you book the whole node, so you can't book just a few cores on them. That means that if you need 130GB RAM and the program is only single threaded, your only option is to book a whole high memory node. The job will look really inefficient, but it's the only way to do it on our system. The example in the plot does not fall into this category though, as it uses only ~15GB of RAM, which you could get by booking 2-3 normal cores.</p>"},{"location":"efficiency_using_sacct/","title":"Efficiency using <code>sacct</code>","text":"<p>There are multiple tools for using your HPC resources efficiently you may need. This page is about using your HPC resources efficiently using the <code>sacct</code> tool.</p> <p>Here is the general strategy to effectively use your HPC resources:</p> Want to see a video? <p>Watch the YouTube video obtain the CPU and memory usage of a job using <code>sacct</code> to see how to do so.</p> <p>Watch the YouTube video Efficient HPC resource use, using Slurm and sacct to see how the reasoning of this strategy works out.</p> <pre><code>flowchart TD\n  obtain_data[Obtain CPU and memory usage of a job]\n  lower_limit_based_on_memory(Book enough memory)\n  limited_by_cpu(For that amount of cores, would runtime by limited by CPU?)\n  lower_limit_based_on_cpu(Increase the number of cores, so that on average, the right amount of CPUs is booked)\n\n  done(Use that amount of cores)\n\n  add_one(Increase the number of cores by one for safety)\n\n  obtain_data --&gt; lower_limit_based_on_memory\n  lower_limit_based_on_memory --&gt; limited_by_cpu\n  limited_by_cpu --&gt; |no| add_one\n  limited_by_cpu --&gt; |yes| lower_limit_based_on_cpu\n  lower_limit_based_on_cpu --&gt; done\n  add_one --&gt; done</code></pre> Why not look at CPU usage? <p>Because CPU is more flexible.</p> <p>For example, imagine a job with a short CPU spike, that can be processed by 16 CPUs. If 1 core has enough memory, use 1 core of memory: the CPU spike will be turned into a 100% CPU use (of that one core) for a longer duration.</p> <p>To obtain the CPU and memory usage of a job using <code>sacct</code>:</p> <pre><code>sacct --format=elapsed,ncpus,ntasks,UserCPU,CPUTime,AveCPU,MaxVMSize,ReqMem -j [job_number]\n</code></pre> <p>for example:</p> <pre><code>sacct --format=elapsed,ncpus,ntasks,UserCPU,CPUTime,AveCPU,MaxVMSize,ReqMem -j 71611\n</code></pre> <p>This will produce output such as this:</p> <pre><code>   Elapsed      NCPUS   NTasks    UserCPU    CPUTime     AveCPU  MaxVMSize     ReqMem\n---------- ---------- -------- ---------- ---------- ---------- ---------- ----------\n  00:00:13         38           00:01.615   00:08:14                          222000M\n  00:00:13         38        1  00:01.615   00:08:14   00:00:00   3227532K\n  00:00:13         38        1   00:00:00   00:08:14\n</code></pre> Need a worked-out example? <p>Here is an example output:</p> <p></p> <pre><code>   Elapsed      NCPUS   NTasks    UserCPU    CPUTime     AveCPU  MaxVMSize     ReqMem\n---------- ---------- -------- ---------- ---------- ---------- ---------- ----------\n  00:00:13         38           00:01.615   00:08:14                          222000M\n  00:00:13         38        1  00:01.615   00:08:14   00:00:00   3227532K\n  00:00:13         38        1   00:00:00   00:08:14\n</code></pre> <p>Book enough memory</p> <p>There were 38 CPUs booked, which provides for 222000 megabyte of memory. The memory used was 3227532 kilobyte, which is around 3227 megabyte. So we only need 3227 megabyte out of 222000 megabyte. <code>3227 / 222000 = 0.014536036 =</code> 1.5% of what we requested. 1.5% of 38 CPUs is 0.6 CPU needed. Hence, booking 1 CPU will provide enough memory</p> <p>For that amount of cores, would runtime by limited by CPU?</p> <p>Yes: we need 2 cores.</p> <p>On average, each of the 38 cores spent 0 seconds (i.e. max 0.049 seconds) working, out of 13 seconds. Using 1 core instead, means that all the work, 0.049 seconds per core for 38 cores can be done in <code>0.049 * 38 =</code> 1.9 core. This means that in practice one books 2 cores.</p> <p>Increase the number of cores by one for safety</p> <p>This would result in 3 cores.</p> <p>Sometimes, however, it is inevitable to use resources inefficiently.</p>"},{"location":"efficiency_using_sacct/#examples","title":"Examples","text":"<p>Here are some examples of how inefficient jobs can look and what you can do to make them more efficient.</p>"},{"location":"efficiency_using_sacct/#inefficient-job-example-1-booking-too-much-cores","title":"Inefficient job example 1: booking too much cores","text":"<pre><code>   Elapsed      NCPUS   NTasks    UserCPU    CPUTime     AveCPU  MaxVMSize     ReqMem\n---------- ---------- -------- ---------- ---------- ---------- ---------- ----------\n  00:00:01         64           00:12.995   00:01:04                             375G\n  00:00:01         64        1  00:12.995   00:01:04   00:00:00   3424140K\n  00:00:01         64        1   00:00:00   00:01:04\n</code></pre> <p>Here booking ? cores is considered okay.</p> <p>Book enough memory</p> <p>There were 64 CPUs booked, which provides for 375 gigabyte of memory. The memory used was 3424140 kilobyte, which is around 3424 megabyte. So we only need 3424 megabyte out of 375000 megabyte. <code>3424 / 375000 = 0.009130667 =</code>0.9% of what we requested. 0.9% of 64 cores is 0.6 core needed. Hence, booking 1 core will provide enough memory</p> <p>For that amount of cores, would runtime by limited by CPU?</p> <pre><code>Yes: we need 2 cores.\n\nOn average, each of the 64 cores spent 0 seconds (i.e. max 0.049 seconds)\nworking, out of 1 second. Using 1 core instead, means that all the work,\n0.049 seconds per core for 64 cores can be done in `0.049 * 64 =` 3.13 core.\nThis means that in practice one books 4 cores.\n</code></pre> <p>Increase the number of cores by one for safety</p> <p>This would result in 3 cores.</p>"},{"location":"efficiency_using_seff/","title":"Efficiency using <code>seff</code>","text":"<p>There are multiple tools for using your HPC resources efficiently you may need. This page is about using your HPC resources efficiently using the <code>seff</code> tool.</p> <p>Here is the general strategy to effectively use your HPC resources:</p> Want to see a video? <p>Watch the YouTube video obtain the CPU and memory usage of a job using <code>seff</code> to see how to do so.</p> <p>Watch the YouTube video Efficient HPC resource use, using Slurm and seff to see how the reasoning of this strategy works out.</p> <pre><code>flowchart TD\n  obtain_data[Obtain CPU and memory usage of a job]\n  lower_limit_based_on_memory(Book enough memory)\n  limited_by_cpu(For that amount of cores, would runtime by limited by CPU?)\n  lower_limit_based_on_cpu(Increase the number of cores, so that on average, the right amount of CPUs is booked)\n\n  done(Use that amount of cores)\n\n  add_one(Increase the number of cores by one for safety)\n\n  obtain_data --&gt; lower_limit_based_on_memory\n  lower_limit_based_on_memory --&gt; limited_by_cpu\n  limited_by_cpu --&gt; |no| add_one\n  limited_by_cpu --&gt; |yes| lower_limit_based_on_cpu\n  lower_limit_based_on_cpu --&gt; done\n  add_one --&gt; done</code></pre> Why not look at CPU usage? <p>Because CPU is more flexible.</p> <p>For example, imagine a job with a short CPU spike, that can be processed by 16 CPUs. If 1 core has enough memory, use 1 core of memory: the CPU spike will be turned into a 100% CPU use (of that one core) for a longer duration.</p> <p>To obtain the CPU and memory usage of a job using <code>seff</code>:</p> <pre><code>seff [job_number]\n</code></pre> <p>for example:</p> <pre><code>seff 12696175\n</code></pre> <p>This will produce output such as this:</p> <pre><code>Job ID: 12696175\nCluster: dardel\nUser/Group: aletyner/aletyner\nState: COMPLETED (exit code 0)\nNodes: 1\nCores per node: 160\nCPU Utilized: 00:00:03\nCPU Efficiency: 0.00% of 1-23:22:40 core-walltime\nJob Wall-clock time: 00:17:46\nMemory Utilized: 4.35 GB\nMemory Efficiency: 3.17% of 137.19 GB (878.00 MB/core)\nThe task which had the largest memory consumption differs by 102.24% from the average task max memory consumption\n</code></pre> Need a worked-out example? <p>.</p> <p>Book enough memory</p> <p>.</p> <p>For that amount of cores, would runtime by limited by CPU?</p> <p>.</p> <p>Increase the number of cores by one for safety</p> <p>.</p> Need another worked-out example? <p>Book enough memory</p> <p>.</p> <p>For that amount of cores, would runtime by limited by CPU?</p> <p>.</p> <p>Increase the number of cores, so that on average the right amount of CPUs are booked</p> <p>.</p> <p>Sometimes, however, it is inevitable to use resources inefficiently.</p>"},{"location":"efficiency_using_seff/#examples","title":"Examples","text":"<p>Here are some examples of how inefficient jobs can look and what you can do to make them more efficient.</p>"},{"location":"efficiency_using_seff/#inefficient-job-example-1-booking-too-much-cores","title":"Inefficient job example 1: booking too much cores","text":"<pre><code>Job ID: 12696175\nCluster: dardel\nUser/Group: aletyner/aletyner\nState: COMPLETED (exit code 0)\nNodes: 1\nCores per node: 160\nCPU Utilized: 00:00:03\nCPU Efficiency: 0.00% of 1-23:22:40 core-walltime\nJob Wall-clock time: 00:17:46\nMemory Utilized: 4.35 GB\nMemory Efficiency: 3.17% of 137.19 GB (878.00 MB/core)\nThe task which had the largest memory consumption differs by 102.24% from the average task max memory consumption\n</code></pre> <p>Here booking 7 cores is considered okay.</p> <p>Book enough memory</p> <p>In this job, only 3.17% of the memory of was used. 3.17% of 160 scheduled cores is 5.072 core. In practice, this will be 6 cores.</p> <p>For that amount of cores, would runtime by limited by CPU?</p> <p>The answer is 'no': we see a CPU efficiency of 0.00% (i.e. 0.0049% or lower). Hence, using reducing the number of cores to 3.17% will still be enough for the CPU.</p> <p>Increase the number of cores by one for safety</p> <p>This means booking 7 cores is recommended.</p>"},{"location":"faq/","title":"Frequently asked questions","text":""},{"location":"faq/#what-is-the-goal-of-this-site","title":"What is the goal of this site?","text":"<p>This site intends to help a user find the resources for his/her needs.</p>"},{"location":"faq/#what-is-new-about-this-site","title":"What is new about this site?","text":"<ul> <li>Combines information from multiple sites.   The best example is the 'Courses' page,   where this site is the first to combine the courses offered by   13 different   course providers.</li> <li>Friendly to contributions. For example, each page has   an 'Edit' button at the top-right corner, making it easy for everyone   (with a GitHub account) to suggest changes</li> <li>Checks link to be valid. Not all resource providers do so:   for them, the 'Courses' page is a useful resource,   as this indicates which of their course links are broken.</li> </ul>"},{"location":"faq/#who-are-meant-with-user","title":"Who are meant with 'user'?","text":"<p>Those that are allowed to use at least one of the resources listed.</p> <p>These are some categories a user can be in:</p> <ul> <li>Anyone</li> <li>Anyone in Europe</li> <li>Anyone in Sweden</li> <li>A researcher based in Sweden</li> <li>A researcher associated to a specific Swedish university</li> <li>A researcher in a specific field</li> </ul>"},{"location":"faq/#what-is-meant-with-resources","title":"What is meant with 'resources'?","text":"<p>Computer-related and associated things, such as:</p> <ul> <li>Computational power, e.g. HPC clusters</li> <li>Storage space, e.g. HPC storage systems</li> <li>A static or interactive website</li> <li>Knowledge how to use these resources listed, i.e. courses</li> <li>Efficiency, i.e. knowing how to use computational power effectively</li> </ul>"},{"location":"faq/#who-is-behind-this-site","title":"Who is behind this site?","text":"<p>SCoRe is behind this site.</p>"},{"location":"faq/#what-is-score","title":"What is SCoRe?","text":"<p>SCoRe is an abbreviation of 'Support for Computational Resources'. One of the purposes of this NBIS unit is to help Swedish life scientists find the resources they need.</p>"},{"location":"other/","title":"Other","text":"<p>There are multiple types of resources you may need. This page is about other resources.</p> Why is this a useful resource? <p>This page is the only page that combines all the other resources of all the different providers.</p> How is this list generated and updated? <p>On a daily basis, the <code>update_content.yaml</code> continuous integration script checks the websites of the course providers and updates the list, using the <code>scoreto</code> R package.</p> My resource is absent! <p>If your resource is absent, please contribute or contact us.</p> My resource can be displayed better! <p>If your resource can be displayed better, please contribute or contact us.</p> How can I read this data is a machine-friendly format? <p>This information can be downloaded as a <code>.csv</code> from the <code>scoreto</code> R package.</p> Name Description User fee Accessible for Center(s) AIDA Data Hub datasets Download datasets from the AIDA Nextcloud Free Everyone EOSC Bulk Data Transfer Transfer data in bulk Unknown EU citizens EOSC Large File Transfer Transfer large files Unknown EU citizens"},{"location":"resources/","title":"Resources","text":"<p>The goal of SCoRe is in its name: 'Support for Computational Resources'. This site intends to help a user find the resources for his/her needs.</p> Need Resource I need to run heavy calculations Compute I need to run heavy calculations efficiently Efficiency I need to store big amounts of data Storage I need a website Web hosting I need to learn how to do something Courses and training I need something else Other I have other needs Contact us"},{"location":"storage/","title":"Storage","text":"<p>There are multiple types of resources you may need. This page is about finding a place to store big amounts of data: it shows a flowchart how to determine the storage resource you can use, followed by an overview of all resources.</p> <p>Before you start, you should probably do a data classification and write a data management plan.</p> <p>Further things to consider include things like keeping a backup, versioning, etc, but the flowchart below can be a useful start. We will happily discuss your needs and help you find something suitable.</p> <pre><code>flowchart TD\n\n  question_need_repo[Do you want to work actively on the data?]\n  question_need_repo --&gt; |Yes| question_heavy_compute\n  question_need_repo --&gt; |No| question_publish_archive\n\n  question_publish_archive[Do you want to publish data?]\n  question_publish_archive --&gt; |Yes| research_field\n  question_publish_archive --&gt; |No| others\n\n  question_heavy_compute[Need heavy compute?]\n  question_heavy_compute --&gt; |Yes| question_hpc_cluster[Cluster]\n  question_heavy_compute --&gt; |No| others\n\n  click question_hpc_cluster \"./compute.md\" \"Compute decision tree\"\n\n  subgraph research_field[\"Research field\"]\n\n    bolin_centre_database[Bolin Centre Database]\n    click bolin_centre_database \"https://bolin.su.se/data\"\n    fega_sweden[FEGA Sweden]\n    click fega_sweden \"https://fega.nbis.se\"\n    gbif_sweden[GBIF Sweden]\n    click gbif_sweden \"https://www.gbif.se\"\n    sll_data_repository[SciLifeLab Data Repository]\n    click sll_data_repository \"https://www.scilifelab.se/data/repository/\"\n    sites_data_portal[SITES Data Portal]\n    click sites_data_portal \"https://data.fieldsites.se/portal/\"\n    sbdi[SBDI]\n    click sbdi \"https://biodiversitydata.se\"\n    ena[ENA]\n    click ena \"https://www.ebi.ac.uk/ena/browser/home\"\n\n    question_research_field[What is your research field?]\n\n    question_research_field --&gt; |Climate| bolin_centre_database\n    question_research_field --&gt; |Genomics or phenomics| q_fega_sweden\n    question_research_field --&gt; |Biodiversity| gbif_sweden\n    question_research_field --&gt; |Biodiversity| sbdi\n    question_research_field --&gt; |Life science| sll_data_repository\n    question_research_field --&gt; |Ecosystems| sites_data_portal\n\n    q_fega_sweden[Sensitive human data?] --&gt; |Yes| fega_sweden\n    q_fega_sweden[Sensitive human data?] --&gt; |No| ena\n  end\n\n  subgraph others[\"Others\"]\n\n    eosc_file_sync_and_share[EOSC File Sync and Share]\n    click eosc_file_sync_and_share \"https://open-science-cloud.ec.europa.eu/services/file-sync-share\"\n    sll[SciLifeLab FAIR Storage]\n    click sll \"https://data.scilifelab.se/services/fairstorage/\"\n    uu[VESTA]\n    click uu \"https://www.uu.se/medarbetare/stod-och-verktyg/it/it-tjanster/tillaggstjanster/vesta\"\n    lu[COSMOS SENS or LUSEC]\n    click lu \"https://www.medarbetarwebben.lu.se/lagring-forskningsdata\"\n    gu[GU TRE]\n    click gu \"https://tre.gu.se\"\n    ki[OneDrive or SciShare]\n    click ki \"https://staff.ki.se/tools-and-support/it-and-telephony/store-and-share-files\"\n\n    question_sync_and_share[\"Want to sync and share files?\"]\n    question_sync_and_share --&gt; |Yes| eosc_file_sync_and_share\n    question_sync_and_share --&gt; |No| question_data_sensitivity\n\n    question_data_sensitivity[\"Is the data sensitive?\"]\n    question_data_sensitivity --&gt; |Yes| local_secure\n    question_data_sensitivity --&gt; |No| sll\n\n    local_secure[Local secure storage]\n    local_secure --&gt; |UU| uu\n    local_secure --&gt; |LU| lu\n    local_secure --&gt; |GU| gu\n    local_secure --&gt; |KI| ki\n\n  end\n</code></pre> Why is this a useful resource? <p>This page is the only page that combines all the storage resources of all the different providers.</p> How is this list generated and updated? <p>On a daily basis, the <code>update_content.yaml</code> continuous integration script checks the websites of the course providers and updates the list, using the <code>scoreto</code> R package.</p> A storage provider is missing! <p>If a storage provider is missing, please contribute or contact us.</p> My storage resource is absent! <p>If your storage resource is absent, please contribute or contact us.</p> My storage resource can be displayed better! <p>If your storage resource can be displayed better, please contribute or contact us.</p> How can I read this data is a machine-friendly format? <p>This information can be downloaded as a <code>.csv</code> from the <code>scoreto</code> R package.</p> HPC storage system name Data sensitivity Data activity User fee Accessible for Center(s) Berzelius Storage Regular Active Free Users of the NSC Berzelius HPC cluster Bolin Centre Database Regular Any Free Climate researchers Center Storage Regular Active Free Users of the NSC HPC clusters Centerstorage nobackup Regular Active Free Users of the LUNARC HPC clusters Crex 1 Regular Active Free Users of the UPPMAX Rackham and Snowy HPC clusters Data Science Platform Any Any Prices Anyone dCache Regular Active Free Swedish researchers DORIS Any Any Free Swedish researchers EOSC File Sync and Share Unsure Unknown Unknown EU citizens FEGA Sweden Sensitive Any Free Swedish researchers working on genotype and phenotype data GBIF Sweden Regular Any Free Swedish researchers working on biodiversity data Klemming Regular Active Free Users of the PDC Dardel HPC cluster Mimer Regular Active Free Users of the C3SE Alvis HPC cluster Nobackup Regular Active Free Users of the HPC2N HPC clusters SciLifeLab Data Repository Regular Any Free Swedish life science researchers SITES Data Portal Regular Any Free Swedish ecosystem researchers Spirula Regular Active Free Swedish data-driven life science researchers Swedish Biodiversity Data Infrastructure Regular Any Free Swedish researchers working on biodiversity data Vesta Sensitive Any Free Uppsala University researchers"},{"location":"web_host/","title":"Web host","text":"<p>There are multiple types of resources you may need. This page is about a place to host a website: it shows a flowchart how to determine the resource you can use, followed by an overview of all resources.</p>"},{"location":"web_host/#selecting-the-right-resource","title":"Selecting the right resource","text":"<pre><code>flowchart TD\n\n  eosc_interactive_notebooks[EOSC Interactive Notebooks]\n  eosc_virtual_machines[EOSC Virtual Machines]\n  scc[Swedish Science Cloud]\n\n  sll_serve[SciLifeLab Serve]\n  eosc_cloud_container_platform[EOSC Cloud Container Platform]\n\n  question_notebook[Do you only need a Jupyter notebook?]\n  question_notebook --&gt; |Yes| eosc_interactive_notebooks\n  question_notebook --&gt; |No| question_deployment\n\n  question_deployment[How do you want to deploy?]\n  question_deployment --&gt; |From a container| question_life_science\n  question_deployment --&gt; |From an image| eosc_virtual_machines\n  question_deployment --&gt; |Custom| scc\n\n  question_life_science[Do you work in life science?]\n  question_life_science --&gt; |Yes| sll_serve\n  question_life_science --&gt; |No| eosc_cloud_container_platform </code></pre> How is this list generated and updated? <p>On a daily basis, the <code>update_content.yaml</code> continuous integration script checks the websites of the course providers and updates the list, using the <code>scoreto</code> R package.</p> My web hosting resource is absent! <p>If your web hosting resource is absent, please contribute or contact us.</p> My web hosting resource can be displayed better! <p>If your web hosting resource can be displayed better, please contribute or contact us.</p> How can I read this data is a machine-friendly format? <p></p> <p>This information can be downloaded as a <code>.csv</code> from the <code>scoreto</code> R package.</p> Web host name Deploy type Data sensitivity User fee Accessible for Center(s) EOSC Cloud Container Platform Containers Unknown Unknown EU citizens EOSC Interactive Notebooks Jupyter notebooks in multiple programming languages such as R and Python Unknown Unknown EU citizens EOSC Virtual Machines Predefined virtual machines Unknown Unknown EU citizens SciLifeLab Serve Containers Regular Free Life science scientists Swedish Science Cloud Any type Regular Free Swedish researchers"},{"location":"logo/","title":"Logo","text":"<p>These are logo's used in this repository.</p> Logo Description Source AIDA Data Hub logo Source EOSC logo Source NAISS logo Source SCoRe logo Source"}]}